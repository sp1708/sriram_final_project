{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Animal Adoption Revisit\n",
    "\n",
    "## 1. <a name=\"1\">Read the dataset</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the dataset into a dataframe, using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "  \n",
    "df = pd.read_csv('../data/review/review_dataset.csv')\n",
    "\n",
    "print('The shape of the dataset is:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a name=\"2\">Exploratory Data Analysis</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We will look at number of rows, columns and some simple statistics of the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's see the data types and non-null values for each column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Print basic statistics for numerical columns\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's separate model features and model target. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Display all columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a name=\"3\">Select features to build the model</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "This time we build a model using all features. That is, we build a classifier including __numerical, categorical__ and __text__ features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab model features/inputs and target/output\n",
    "# can also grab less numerical features, as some numerical data might not be very useful\n",
    "numerical_features = ['Age upon Intake Days', 'Age upon Outcome Days']\n",
    "\n",
    "# dropping the IDs features, RescuerID and PetID here \n",
    "categorical_features = ['Sex upon Outcome', 'Intake Type',\n",
    "       'Intake Condition', 'Pet Type', 'Sex upon Intake']\n",
    "\n",
    "# from EDA, select the text features\n",
    "text_features = ['Name', 'Found Location', 'Breed', 'Color']\n",
    "    \n",
    "model_features = numerical_features + categorical_features + text_features\n",
    "model_target = 'Outcome Type'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check our target distribution with a bar chart. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning numerical features "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Plot bar chart for all the numerical features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find the min and max for all numerical features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check missing values for these numerical features."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning categorical features \n",
    "\n",
    "Let's also examine the categorical features.\n",
    "\n",
    "- Find missing values\n",
    "- Convert all categorical and text features to str type using `astype('str')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning text features \n",
    "\n",
    "Also a good idea to look at the text fields. Text cleaning can be performed here, before train/test split, with less code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in text_features:\n",
    "    print(c)\n",
    "    print(df[c].unique()) #value_counts())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We re-use the helper functions from the 'Text processing' notebook. Complete the functions below\n",
    "\n",
    "__Warning__: cleaning stage can take a few minutes, depending on how much text is there to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare cleaning functions\n",
    "import re, string\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stop_words = [\"a\", \"an\", \"the\", \"this\", \"that\", \"is\", \"it\", \"to\", \"and\"]\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "def preProcessText(text):\n",
    "    # fill in here\n",
    "    \n",
    "    return text\n",
    "\n",
    "def lexiconProcess(text, stop_words, stemmer):\n",
    "    # fill in here\n",
    "    \n",
    "    return text\n",
    "\n",
    "def cleanSentence(text, stop_words, stemmer):\n",
    "    return lexiconProcess(preProcessText(text), stop_words, stemmer)\n",
    "\n",
    "# Clean the text features\n",
    "for c in text_features:\n",
    "    print('Text cleaning: ', c)\n",
    "    df[c] = [cleanSentence(item, stop_words, stemmer) for item in df[c].values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cleaned text features are ready to be vectorized after the train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: more exploratory data analysis might reveal other important hidden atributes and/or relationships of the model features considered. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a name=\"4\">Training and test datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "- Split our dataset into training (90%) and test (10%) subsets using sklearn's [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Target balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check the target distribution in the training and test set. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fix the imbalance only in training set. We shouldn't change the validation and test sets, as these should follow the original distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a name=\"5\">Data processing with Pipeline and ColumnTransformer</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's build a more complex pipeline today. We first build separate pipelines to handle the numerical, categorical, and text features, and then combine them into a composite pipeline along with an estimator, a [Decision Tree Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) here.\n",
    "\n",
    "   * For the numerical features pipeline, the __numerical_processor__ below, we impute missing values with the mean using sklearn's SimpleImputer, followed by a MinMaxScaler (don't have to scale features when using Decision Trees, but it's a good idea to see how to use more data transforms). If different processing is desired for different numerical features, different pipelines should be built - just like shown below for the two text features.\n",
    "   \n",
    "   \n",
    "   * In the categoricals pipeline, the __categorical_processor__ below, we impute with a placeholder value (no effect here as we already encoded the 'nan's), and encode with sklearn's OneHotEncoder. If computing memory is an issue, it is a good idea to check categoricals' unique values, to get an estimate of many dummy features will be created by one-hot encoding. Note the __handle_unknown__ parameter that tells the encoder to ignore (rather than throw an error for) any unique value that might show in the validation/and or test set that was not present in the initial training set.\n",
    "  \n",
    "   \n",
    "   * And, finally, also with memory usage in mind, we build two more pipelines, one for each of our text features, trying different vocabulary sizes.\n",
    "   \n",
    "The selective preparations of the dataset features are then put together into a collective __ColumnTransformer__, to be finally used in a __Pipeline__ along with an estimator. This ensures that the transforms are performed automatically on the raw data when fitting the model and when making predictions, such as when evaluating the model on a validation dataset via cross-validation or making predictions on a test dataset in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "### COLUMN_TRANSFORMER ###\n",
    "##########################\n",
    "\n",
    "# Preprocess the numerical features\n",
    "numerical_processor = Pipeline([\n",
    "    ('num_imputer', SimpleImputer(strategy='mean')),\n",
    "    ('num_scaler', MinMaxScaler()) # Shown in case is needed, not a must with Decision Trees\n",
    "                                ])\n",
    "                  \n",
    "# Preprocess the categorical features\n",
    "categorical_processor = Pipeline([\n",
    "    ('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # Shown in case is needed, no effect here as we already imputed with 'nan' strings\n",
    "    ('cat_encoder', OneHotEncoder(handle_unknown='ignore')) # handle_unknown tells it to ignore (rather than throw an error for) any value that was not present in the initial training set.\n",
    "                                ])\n",
    "\n",
    "# Preprocess 1st text feature\n",
    "text_processor_0 = Pipeline([\n",
    "    ('text_vect_0', CountVectorizer(binary=True, max_features=50))\n",
    "                                ])\n",
    "\n",
    "# Preprocess 2nd text feature (larger vocabulary)\n",
    "text_precessor_1 = Pipeline([\n",
    "    ('text_vect_1', CountVectorizer(binary=True, max_features=150))\n",
    "                                ])\n",
    "\n",
    "# Combine all data preprocessors from above (add more, if you choose to define more!)\n",
    "# For each processor/step specify: a name, the actual process, and finally the features to be processed\n",
    "data_preprocessor = ColumnTransformer([\n",
    "    ('numerical_pre', numerical_processor, numerical_features),\n",
    "    ('categorical_pre', categorical_processor, categorical_features),\n",
    "    ('text_pre_0', text_processor_0, text_features[0]),\n",
    "    ('text_pre_1', text_precessor_1, text_features[1])\n",
    "                                    ]) \n",
    "\n",
    "### PIPELINE ###\n",
    "################\n",
    "\n",
    "# Pipeline desired all data transformers, along with an estimator at the end\n",
    "# Later you can set/reach the parameters using the names issued - for hyperparameter tuning, for example\n",
    "pipeline = Pipeline([\n",
    "    ('data_preprocessing', data_preprocessor),\n",
    "    ('dt', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Visualize the pipeline\n",
    "# This will come in handy especially when building more complex pipelines, stringing together multiple preprocessing steps\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a name=\"6\">Train and tune a classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's first train and test the above composite pipeline on the train and the test sets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Get train data to train the pipeline\n",
    "X_train = train_data[model_features]\n",
    "y_train = train_data[model_target]\n",
    "\n",
    "# Fit the Pipeline to training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the train dataset\n",
    "train_predictions = pipeline.predict(X_train)\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Accuracy (training):\", accuracy_score(y_train, train_predictions))\n",
    "\n",
    "# Get test data to test the pipeline\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted pipeline to make predictions on the test dataset\n",
    "test_predictions = pipeline.predict(X_test)\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Accuracy (test):\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Tuning\n",
    "\n",
    "We next use sklearn's [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to look for hyperparameter combinations to improve the accuracy on the test set (and reduce the generalization gap). As GridSearchCV does cross-validation train-validation split internally, \n",
    "our data transformers inside the Pipeline context will force the correct behavior of learning data transformations on the training set, and applying the transformations to the validation set when cross-validating, as well as on the test set later when running test predictions.\n",
    "\n",
    "Also, Pipeline's steps names give easy access to hyperparameters for hyperparameter tuning while cross-validating. Parameters of the estimators in the pipeline can be accessed using the __estimator__ __ __parameter__ syntax. Note the __double underscores__ connecting the __estimator__ and __parameter__!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "### PIPELINE GRID_SEARCH ###\n",
    "############################\n",
    "\n",
    "# Parameter grid for GridSearch\n",
    "param_grid={'dt__max_depth': [100, 200, 300],#, 50, 75, 100, 125, 150, 200, 250], \n",
    "            'dt__min_samples_leaf': [5, 10, 15],#, 25, 30],\n",
    "            'dt__min_samples_split': [2, 5, 15]#, 25, 30, 45, 50]\n",
    "           }\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, # Base model\n",
    "                           param_grid, # Parameters to try\n",
    "                           cv = 5, # Apply 5-fold cross validation\n",
    "                           verbose = 1, # Print summary\n",
    "                           n_jobs = -1 # Use all available processors\n",
    "                          )\n",
    "\n",
    "# Fit the GridSearch to our training data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "print(grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the best model out of GridSearchCV\n",
    "\n",
    "- Fit the best model to the train data once more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. <a name=\"7\">Test the classifier</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Now we test the best model with the best parameters on \"unseen\" data (our test data).\n",
    "\n",
    "Before that, let's first see how the model works on the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Use the fitted model to make predictions on the train dataset\n",
    "train_predictions = classifier.predict(X_train)\n",
    "\n",
    "print('Model performance on the train set:')\n",
    "print(confusion_matrix(y_train, train_predictions))\n",
    "print(classification_report(y_train, train_predictions))\n",
    "print(\"Train accuracy:\", accuracy_score(y_train, train_predictions))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's evaluate the performance of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "# Get test data to test the classifier\n",
    "X_test = test_data[model_features]\n",
    "y_test = test_data[model_target]\n",
    "\n",
    "# Use the fitted model to make predictions on the test dataset\n",
    "# Test data going through the Pipeline it's first imputed (with means from the train), scaled (with the min/max from the train data), and finally used to make predictions\n",
    "test_predictions = classifier.predict(X_test)\n",
    "\n",
    "print('Model performance on the test set:')\n",
    "print(confusion_matrix(y_test, test_predictions))\n",
    "print(classification_report(y_test, test_predictions))\n",
    "print(\"Test accuracy:\", accuracy_score(y_test, test_predictions))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Tasks\n",
    "\n",
    "Try ensemble models such as Random Forests, along with RandomizedSearchCV to speed up the hyperparameter tuning task. (check slides)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eba38789ab565d76f074e8fa97ecc7da63eb4a5e1ba28cc348f16f5285783ca7"
  },
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
