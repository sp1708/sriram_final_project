{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"0\">Wine Data Exercises</a>\n",
    "\n",
    "In this notebook, we will review basic steps of exploratory data analysis following the example in the EDA-PIPELINE,ipynb example. We will work with the wine data set __winequality-white.csv__ provided in the __data__ folder. \n",
    "\n",
    "__Dataset schema:__ \n",
    "   - fixed acidity\n",
    "   - volatile acidity\n",
    "   - citric acid\n",
    "   - residual sugar\n",
    "   - chlorides\n",
    "   - free sulfur dioxide\n",
    "   - total sulfur dioxide\n",
    "   - density\n",
    "   - pH\n",
    "   - sulphates\n",
    "   - alcohol\n",
    "\n",
    "   Output variable (based on sensory data): \n",
    "   - quality (score between 0 and 10)\n",
    "\n",
    "\n",
    "You will follow the same steps in the _EDA-PIPELINE.ipynb_ notebook to complete the wine quality classification tasks. The section headers are provided,  and you will need to copy and paste the code over, while adding necessary modifications. \n",
    "\n",
    "1. <a href=\"#1\">Overall Statistics</a>\n",
    "2. <a href=\"#2\">Select Feature Columns</a>\n",
    "3. <a href=\"#3\">Basic Plots</a>\n",
    "4. <a href=\"#4\">Impute Missing Values</a>\n",
    "5. <a href=\"#5\">Preparing Training and Test Datasets</a>\n",
    "6. <a href=\"#6\">Data Processing with Pipeline</a>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Copy multiple cells from one notebook to another__\n",
    " - Select Cell and press Esc to go to command mode.\n",
    " - Hit Shift + Up or Shift + Down to select multiple cells.\n",
    " - Copy with Ctrl/CMD + C.\n",
    " - Paste with Ctrl/CMD + V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. <a id=\"=1\">Overall Statistics</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's read the dataset into a dataframe, using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. <a id=\"2\">Select Feature Columns</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "Let's separate model features and model target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. <a id=\"3\">Basic Plots</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### Bar Plots and Histograms\n",
    "\n",
    "In this section, we examine our data with plots. Important note: These plots ignore null (missing) values. We will learn how to deal with missing values in the next section.\n",
    "\n",
    "\n",
    "__Bar plots__: These plots show counts of categorical data fields. __value_counts()__ function yields the counts of each unique value. It is useful for categorical variables.\n",
    "\n",
    "First, let's look at the distribution of the model target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Histograms:__ Histograms show distribution of numeric data. Data is divided into intervals, aka, \"buckets\" or \"bins\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Outliers \n",
    "\n",
    "For the wine dataset, you may select 1-2 features and remove the outliers in these features. (Note what happens if you perform this operation on all features.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scatter Plot and Correlation Matrix\n",
    "\n",
    "We can plot the scatter plot for two selected numeric features.\n",
    "\n",
    "We plot the correlation matrix. Correlation scores are calculated for numerical fields. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. <a id=\"4\">Impute Missing Values</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "### Impute (fill-in) missing values with .fillna()\n",
    "\n",
    "\n",
    "Rather than dropping rows (data samples) and/or columns (features), another strategy to deal with missing values would be to actually complete the missing values with new values: imputation of missing values.\n",
    "\n",
    "__Imputing Numerical Values:__ The easiest way to impute numerical values is to get the __average (mean) value__ for the corresponding column and use that as the new value for each missing record in that column. \n",
    "\n",
    "An elegant way to implement imputation is using sklearn's __SimpleImputer__, a class implementing .fit() and .transform() methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. <a id=\"5\">Preparing Training and Test Datasets</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "We split our dataset into training (90%) and test (10%) subsets using sklearn's [__train_test_split()__](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. <a id=\"6\">Data Processing with Pipeline</a>\n",
    "(<a href=\"#0\">Go to top</a>)\n",
    "\n",
    "In a typical machine learning workflow you will need to apply data transformations, like imputation and scaling shown here, at least twice. First on the training dataset with __.fit()__ and __.transform()__, when preparing the data to training the model. And again, on any new data you want to predict on, with __.transform()__. Scikit-learn [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) is a tool that simplifies this process by enforcing the implementation and order of data processing steps. \n",
    "\n",
    "We build a pipeline to impute the missing values with the mean using sklearn's SimpleImputer, scale the numerical features to have similar orders of magnitude by bringing them into the 0-1 range with sklearn's MinMaxScaler, and finally train a KNN estimator on the imputed and scaled dataset. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "eba38789ab565d76f074e8fa97ecc7da63eb4a5e1ba28cc348f16f5285783ca7"
  },
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
